
\chapter{IMAX Hardware}

\section{Background}

IoT，ビッグデータ，AI，ロボットが注目される一方，持続的発展に必須の低電力計
算基盤技術は進展速度が極めて遅い．主因は，半導体微細化限界により表面化した消
費電力−プログラマビリティ間の強いトレードオフ関係にある．JST低炭素社会戦略
センター(LCS)の報告書は，CPUやGPGPU等ノイマン型に依存する状況が続けば，2050
年にはデータセンターの消費電力が供給可能電力を上回ると予測する．データセンター
集中処理にせよエッジコンピューティングにせよ，トレードオフを精査し，低電力計
算基盤の導入を進めなければ，AIやブロックチェインを柱とする次世代持続的社会シ
ステムは画餅に帰す．様々な物理現象を利用する数多のアナログ型計算機構がサイエ
ンスとして探求される中，大規模化・安定運用に要する電力も含め，次世代低電力計
算基盤に至るほぼ唯一の道は，プログラムをデータフローに細分化しハードウェア
に写像して電力効率を２桁改善可能な非ノイマン型の汎用性・プログラマビリティ向
上である．

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.70\textwidth]{CGRA0.eps}
\caption{\label{cgra0}CPU/GPUとCGRAの命令実行モデルの違い}
\end{figure}

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.60\textwidth]{gpu.eps}
\caption{\label{gpu}CPU/GPUとCGRAのメモリアクセスの違い}
\end{figure}

機械語命令が演算器とメモリを逐次制御するノイマン型計算基盤は，半導体微細化と
並列処理高度化の両輪が性能向上を支えてきた．前者は物理限界に近付き後者が追求
された．GPGPUは従来型プログラマビリティを維持しつつ並列処理を行うために，大
規模レジスタファイルとマルチスレッディングによる1000サイクルを超える主記憶遅
延隠蔽機能を備える．ただし，主記憶参照順序を規則化するチューニングが難しく，
要求性能達成のために過大な演算能力と電力を必要とする．一方，1982 年に
H.T.Kung らが提唱したCoarse Grain Reconfigurable Architecture(CGRA)は，最内
ループの命令列を２次元配置の演算器ネットワークに写像して主記憶データを流し込
む非ノイマン型である（図\ref{cgra0},\ref{gpu}）．多くの国内企業が着手するも
プログラマビリティの壁により撤退する中，Google社のTPUやWave computing社のDPU
はAI応用に絞り高効率化に成功している．研究室では2008年にVLIW互換のLAPPを開発
し，その後，64組のニアメモリ高機能演算器と多重ループの統合制御により面積効率
(演算性能/面積)を飛躍的に高めたIMAXを開発した．多数のコアをSIMD動作させる
GPGPU方式が，メモリ参照を連続アドレスに統合するコアレシングと豪華なメモリバ
スを必要とするのに対し，本方式は多数の演算器を縦に多数連結することでメモリバ
スの軽量化と低電力化を達成している．

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.60\textwidth]{cgra.eps}
\caption{\label{cgra}CGRAの理想的動作}
\end{figure}

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.98\textwidth]{APPL.eps}
\caption{\label{appl1}複雑化するアプリケーション}
\end{figure}

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.98\textwidth]{LIST.eps}
\caption{\label{list1}今後CGRAに要求される機能}
\end{figure}

CGRAは膨大な演算を一度に実行する仕組みであるため，効率よく利用するには，図
\ref{cgra}のように，直前の演算結果の主記憶への追い出し，演算，および，次の演
算に必要なデータの主記憶からの取り込みをオーバラップさせ，演算器を稼働し続け
ることが極めて重要である．また，図\ref{appl1}のように，アプリケーションは急
速に複雑化しており，CGRAには，図\ref{list1}に示す多くの機能が要求されている．

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.98\textwidth]{CGRA1.eps}
\caption{\label{cgra1}VPPからIMAXに至る命令実行モデルの歴史}
\end{figure}

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.98\textwidth]{HISTORY.eps}
\caption{\label{history1}VPPからIMAXに至る主要機能の変遷}
\end{figure}

IMAXに至る経緯を図\ref{cgra1},\ref{history1}に示す．脱IBM互換の(a)並列ベクト
ルVPPシリーズでは，海外メーカーがスーパースカラに傾く中，電力効率最大化のた
めVLIWを提案し採用された．VLIW+ベクトル演算機構は，メモリパイプMと演算パイプ
Eのバースト動作がCGRAに酷似するものの，プログラムに対する構造的制約がない特
長を有する．(b)OROCHIは，命令デコーダDがVLIWバッファBにARM命令を動的分解投入
し，キャッシュCを共有するVLIW共用型低電力スーパースカラである．VLIWの2次元拡
張技術はCGRA構成技術の基礎であり，初のVLIW互換CGRAである(c)LAPPが継承した．
(d)EMAX以降は，演算器EとローカルメモリLMのリング接続によるLM最大再利用や，明
示的制御によるデータフロー干渉排除等により，離散ステンシル計算やグラフ処理の
高効率化を達成した．Google社のTPU (整数演算のみ)と同時期の(e)IMAXでは，物理1
列x64行を論理4列x64行に仮想化する多重実行を考案し，LMの最大再利用と併せて，
累算型浮動小数点積和演算Eの面積効率(演算性能/面積)を劇的に改善(28nm想定で組
込みGPGPU比250倍)した．(f)Tandem CGRAは，仮想化機構によりプログラマビリティ
を向上させる構成である．

\clearpage

\section{History of CGRA}

\begin{figure}[htbp]
\center
\epsfile{file=CGRAHIST1.eps,width=0.98\textwidth}
\caption{\label{cgra2}LAPPからIMAXに至る演算器ネットワークの変化}
\end{figure}

図\ref{cgra2}に，当グループが開発してきたCGRAの1行分の構造を示す．LAPPはVLIW
互換CGRAであったものの，FIFOにより構成される各行のローカルメモリ（LMM）容量
は高々16要素x4列と小さく，次数（1辺の長さ）が大きなステンシル計算には対応で
きなかった．EMAX2では，VLIW互換を放棄する代わりに各演算器に専用の中容量LMMと
FIFOを配置し，多くのステンシル計算を写像できるようにした．ただし，LMM数を増
やしたことにより行間レジスタ数が32本に大幅増加した．このため，レジスタから演
算器出力までに4サイクルを要した．EMAX4では，さらにグラフ処理に必要となるトラ
ンザクション機構を追加した．EMAX2からEMAX4への変更点は以下の通りである．

\begin{itemize}\parskip 0pc \itemsep 0pc
\item 複数条件の組合せによる条件付実行の追加
\item トランザクション機能の追加
\item 各段と外部メモリの間のデータバスを64bit*1から32bit*4に拡張
\end{itemize}

EMAX5/L2では，SIMD演算機構を追加するとともに，ALUとEAGの入力レジスタを共用と
することで行間レジスタ数を16本に半減し，レジスタから演算器出力までを2サイク
ルとした．また，外部メモリバス-LMM間スループットを向上させた．EMAX5/ZYNQでは，
FIFO初期化オーバヘッドを削減するためにFIFOを削除し，代わりに，外部メモリバス
から複数列のLMMへ同時に書き込むパスを追加した．これにより，複数の外部メモリ
バスを使用して多数のLMMを同時に初期化することが可能となった．EMAX4から
EMAX5/ZYNQへの変更点は以下の通りである．

\begin{itemize}\parskip 0pc \itemsep 0pc
\item 単精度浮動小数点演算を32bit*2演算へSIMD化
\item 32bit整数演算を32bit*2整数演算へSIMD化
\item 8/16bitメディア演算を4/2倍幅SIMDから8/4倍幅SIMDへ増強
\item 各UNITのローカルメモリ（LMM）バス幅を32bit*1から64bit*4に拡張し，
LMM-外部メモリ間のスループットを8倍に拡張
\item 各段と外部メモリの接続を32bit*4*1チャネルから64bit*4*4チャネルに拡張し，
各段-外部メモリ間のスループットを8倍に拡張（conf/regv/lmmi初期化オーバヘッド
を1/8に削減）
\item 外部メモリ直接参照のためのDual-Port-LMMの採用およびCGRA基本構成の変更
\item 最大3UNITのLMMから3入力SIMDへデータ供給しつつLMMに出力できるUNIT間バスネットワーク
\item UNITあたりの伝搬レジスタ数を8から4に削減し，代わりに伝搬レジスタか
ら演算器へはクロスバスイッチを配置
\item LMMの出力を水平方向に伝搬するFIFOを削除し，代わりに外部メモリバスから
同一段のLMMへブロードキャストする機構を装備（FIFOの初期化が不要）
\end{itemize}

IMAXでは，CGRAの一般的な欠点である配線の多さ，および，自己ループ演算時の演算
回路使用率の低さを改善するために，水平方向の演算には互いに依存関係がないこと
を利用し，複数列の演算機能を1列に束ねるマルチスレッディングを導入した．論理
的には複数列の演算機能（論理UNIT）を実現しつつ，物理的には1列の演算器群によ
り実装することにより，演算器数・配線量削減，演算器使用効率向上，水平方向ブロー
ドキャストの不要化，LMM冗長利用の不要化が可能となった．また，CPUへの接続方法
をAXI-SLAVEに変更し，LMMへの書き込みは，UNIT毎に設けたvAddr-rangeに従い，各
UNITが自律的に取り込む方法に変更した．これにより，垂直方向ブロードキャストも
可能になった．さらに，UNITの出力を入力に戻すパスを追加することにより，同一
LMMに対するread-modify-writeを可能とし，畳み込み演算や逆行列計算に必要となる
LMMアキュムレートの写像を可能とした．なお，複数行のLMMに対して連続する
vAddr-rangeを設定しておき，複数のLMMに対して同一ストアを行うことにより，アキュ
ムレート空間を数倍に増やすことも可能となった．EMAX5/ZYNQからIMAXへの変更点は
以下の通りである．

\begin{itemize}\parskip 0pc \itemsep 0pc
\item EMAX5/ZYNQとのプログラム互換性を維持しながら物理演算器数およびLMM数を
削減
\item AXI-SLAVE化，lmringの8行8列構成，および，垂直方向ブロードキャスト機能
によるホスト主記憶⇔LMM転送効率の向上
\item 同一LMMを使用するread-modify-write機能とアキュムレート機能
\item 同一LMMをダブルバッファとして使用するタンデム機能 (FFTと多段マージソートが使用)
\item デュアルポートLMMを利用したロード結果比較およびアドレス更新機能 (多段マージソートと疎行列積が使用)
\item 要素に位置情報を付加して圧縮した疎行列どうしの行列積演算機能
\item マルチチップ対応およびマルチチップへの写像も含めた多重ループ一括実行機能
\item バースト長が最大8(256bit幅の場合)であるAXI3のDMA-READ高速化のための複
数トランザクション単一化(AXI4化)およびバッファリング機能
\item デュアルEAGを利用した非8バイト境界からの64bitロード
\item SIMD化を容易にする，32bit/8bitロードデータの64bitレジスタ拡張
\item 動的DMA連結による，複数LMMに対する連続データ転送
\end{itemize}

\section{Policy of IMAX}

CGRAの課題は，（１）二次元格子構造のまま実装すると長距離配線や配線混雑が動作
周波数および面積効率の低下をもたらすため省面積化の方策が必要であること，（２）
浮動小数点アキュムレート演算やローカルメモリに対するread-modify-writeのよう
に本質的に複数サイクルを要する命令があってもパイプライン実行を妨げない仕組み
が必要であること，（３）最内ループの高速化だけではベクトル長の短い行列積や畳
み込み演算の高速化に向かないため，起動オーバヘッド削減のための多重ループ一括
実行機能が必要であること，（４）ローカルメモリの再利用効率を上げて外部メモリ
参照を削減するために，静的解析が困難なデータ参照パターンの変動に追随できる命
令写像位置シフト機能が必要であること，（５）高機能なDMAマスタ機能の内蔵を必
要とせず簡素なDMAスレーブメモリとして高効率を達成できること，（６）外部メモ
リバス（AXI）の増設を必要とせず，スレーブメモリの機能を生かしたカスケーディ
ングによりスケーラブルな性能を容易に得られることである．

IMAXの第１のポイントは，CGRAの名称により一般に知られている２次元構成とは大き
く異なり，低コストが要求されるIoTエッジ向けに物理構造を１次元（リニアアレイ）
としつつ高性能を維持した点である．第２のポイントは，メモリ機能を内部に融合し，
ホストからメモリとして使用するため，通常のアクセラレータがメモリ間転送オーバ
ヘッドに苦しむのに対して優位な点，および，メモリであることを利用したカスケー
ド接続が容易な点である．第３のポイントは，メモリ機能と演算機能を一体的に取り
扱う直感的プログラミングのために様々なデータパスを備え，特に，通常のCGRAが不
得意とする，同一ローカルメモリに対するread-modify-writeや多重ループ制御をシ
ンプルな構造により可能とした点である．第４のポイントは，データパスの工夫によ
り，FPGAのような探索的コンパイルを必要とせず，コンパイル速度が高速な点である．

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.98\textwidth]{EMAX6EXP.eps}
\caption{\label{exp}必要とされるメモリ参照パターンと演算}
\end{figure}

IMAXには，1列x64行の物理構造に対して最大10命令（2演算x3stage + ロードx2 + ス
トアx2）x4列x64行＝2560命令を一度に写像できる．IMAXは基本unitを1列x64行(後述)
配置する物理構造を有するものの，プログラミング時のハードウェアモデルは，4列
x64行である．各unitは3段パイプラインの3入力単精度浮動小数点加減乗算器を2組備
え，64ビット幅のレジスタを使用して２つの演算を同時実行する．また，パイプライ
ンの初段は8/16/32ビット単位の各種マルチメディア演算および固定小数点演算，中
段は論理演算，後段はシフト演算等を行うことができる．各unitは64KBデュアルポー
トメモリを備えており，図\ref{exp}に示す様々なメモリ参照パターンに対応できる．
(a)は，1行のローカルメモリ（LMM）に格納されたA,B,Cを4ワードずつ読み出し，4つ
の演算器によりSIMD演算を行った後，演算結果Dを次段のLMMに格納する組を2セット
写像した状態である．A,B,Cは外部メモリから供給され，Dは外部メモリに取り出され
る．(b)は，縦方向に畳み込み演算を行う写像である．デュアルポートメモリから各
列の演算器にAとBを供給し，4系統の畳み込み演算を行っている．(c)は，LMMに4系統
のDを累算する写像である．一般的なCGRAとは異なり，全体としてはデータを下流へ
伝搬しつつ，累算を行うこともできる．(d)は，同様にLMMに4系統のDを累算する写像
である．ただし，格納先が固定され，LMMへのread-modify-writeにより，何度も更新
を行う場合に対応する．(e)はUNIT内多入力積和演算向けの拡張機能である．(f)は疎
行列の行列積やマージソートである．

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.98\textwidth]{CGRAPAT.eps}
\caption{\label{fig:cgrapat}行列積の演算方向}
\end{figure}

例えば行列積の場合，図\ref{fig:cgrapat}に示すように，最適な累算方向は配列Bの
形状によって異なる．累算方向が縦(a)の場合，図\ref{exp}(b)および図
\ref{exp}(c)が必要となる．累算方向が横の(b)は配列Cへのストアが不連続となるた
め最適ではない．同様に，累算方向が縦の(c)は配列Cへのストアが不連続となるため
最適ではない．複数行のBを束ねる(d)の場合，図\ref{exp}(d)または図\ref{exp}(e)
が必要となる．改めて見ると，(a)では配列A複数行の垂直方向ブロードキャスト，
(d)では転置後の配列B複数行の垂直方向ブロードキャストを必要とする点が異なるも
のの，配列Cへのストアは連続領域となる．

\section{省面積化のための列方向マルチスレッディング}

\begin{figure}[htbp]
\center
\includegraphics[angle=0,origin=b,width=0.98\textwidth]{unit.eps}
\caption{\label{fig:unit}Column multithreading for small footprint and
bubble-free execution}
\end{figure}

課題（１）と（２）に対応するために，列方向マルチスレッディングを導入した．図
\ref{fig:unit}(a)に，導入前の一般的なunit構成を示す．一般にCGRAは，前後の演
算との同期コストを削減するため，各演算器が毎サイクル演算結果を出力する前提で
設計される．このため，浮動小数点アキュムレートなど演算時間の長い機能をパイプ
ライン化することができず面積効率が悪化する．また，LMMから多数の読み出しを行
うために，同一内容を保持する複数のLMMを配置して多ポート化する必要があり，面
積効率悪化の要因となる．図\ref{fig:unit}(b)は，マルチスレッディング（W=4）適
用後のunit構成である．1つのunitが4サイクルを使用して論理的に(a)と同じ機能を
実現する．Reg\#0-15を選択する16-to-1セレクタを含めて演算器を4段パイプライン
化し水平方向4個の演算を時分割実行すると，浮動小数点アキュムレートが存在して
も同一性能を維持でき，面積効率を4倍に改善できる．ただし，(a)では1つのunitに
おける演算遅延時間が2サイクルであるのに対し，(b)では8サイクルである．また，4
サイクルかけて出そろう前段unitの演算結果やLMM読み出しデータの全てを次段unit
が参照するために，Reg\#0-15のダブルバッファリングが必要となる（grp.Aおよび
grp.B）．LMMに関しては，2ポートLMMの4サイクル動作により8ポート化すると，水平
方向4個のLMMに全て重複がある場合，面積効率を4倍に改善できる．重複がない場合
はLMMの容量を分割使用するためメモリ本体の効率は変わらないものの，アドレス生
成器は削減できる．AXI-WRITE-IFおよびAXI-READ-IFは，演算用データパスがLMMの
load/storeポートを使わないサイクルを利用してLMMを参照する．

\begin{figure}[htbp]
\center
\epsfile{file=CGRAHIST2.eps,width=0.98\textwidth}
\caption{\label{cgra3}IMAXにおけるレジスタダブルバッファリングの重要性}
\end{figure}

図\ref{cgra3}に，IMAXにおけるダブルバッファの重要性を示す．(a)は4列構成の一
般的なCGRAにおけるメモリと演算器の関係である．メモリの3箇所から各々4ワードを
読み出し，並列動作する4演算器の出力(4ワード分)をメモリに書き込むために，
3-read 1-writeメモリが必要である．4段パイプライン構成の浮動小数点演算器の場
合，アドレス間に依存関係がなければスループットは低下しないものの，累算のよう
に依存関係があると，入力が自身の演算結果を待ち合わせるためパイプラインが停止
し，性能は1/4に低下する．パイプラインを止めないためには，(b)のように1演算器
のみを用いて4way-マルチスレッディング化すればよい．ただし，演算器は削減でき
ても，メモリのポート数は削減できない．(c)はポート数削減のためにSIMD-LDを復活
させた構成である．プログラムはSIMD-LD, SIMD-FMA, SIMD-STの組として記述する．
ハードウェアは，4サイクルのうち3サイクルを使用して3箇所からSIMD-LDによる読み
出しを行う．一旦，ダブルバッファに保存することで，4サイクルの全てにおいて各
演算に必要な3入力が常時参照可能となる．マルチスレッディングによりSIMD-FMAは
時分割実行され，演算結果が順にメモリに格納される．すなわち，SIMD-FMAと
SIMD-STは実際にはSIMD動作しないものの，ハードウェアの性能は100\%発揮できる．

\begin{figure}[htbp]
\center
\includegraphics[angle=0,origin=b,width=0.60\textwidth]{rmw.eps}
\caption{\label{fig:rmw}Read-modify-write operation}
\end{figure}

図\ref{fig:rmw}にread-modify-write operationとunitの対応を示す．論理的に4列
分のread-modify-writeが物理的には単一unitに写像される．前段の演算器による累
算結果（sum0-3）が一旦unit下端のレジスタに送られ，load結果（o0-3）とタイミン
グを合わせて演算器入力へフィードバックされ，加算結果（s0-3）がLMMにstoreされ
る．

\section{起動オーバヘッド削減のための多重ループ一括実行}

\begin{figure}[htbp]
\center \includegraphics[angle=0,origin=b,width=0.80\textwidth]{loop.eps}
\caption{\label{fig:loop}Multilevel loop control}
\end{figure}

課題（３）に対応するために，多重ループ一括実行を導入した．CGRAはバースト演算
起動前に命令写像とレジスタ初期化を必要とするため，なるべくバースト演算時間を
長くし起動オーバヘッドを減らしたい．しかし，バースト演算時間はLMMに格納可能
なデータ量により決まり，LMMの容量にはパイプライン演算器の動作周波数を落さな
い上限がある．経験的に3ステージからなる単精度浮動小数点演算パイプラインと釣
り合うLMMは64KB程度（かつてのベクトル演算機構もベクトルレジスタ1本あたり最大
16KB程度）であり，64KBに格納できるword(4B)数で割った16Kサイクルがバースト演
算時間の上限となる．Lightfield (LF)では入力が8K画像のためLMMを十分使い切るこ
とができる一方，Matrix Multiplication (MM)はバースト演算時間に対応するcolが
M/W（M=480,W=4の場合120），Convolutional Neural Network (CNN)は同様にcolが
M-2（M=242の場合240）と極めて短い．LMMを目一杯使うため，MMは回転数GRP=8のrow
ループを外側に挿入した二重ループとしバースト演算時間をM/WxGRP=960に，CNNも同
様にGRP=8により(M-2)xGRP=1920に増加させ，ハードウェアにも多重ループ一括実行
機構を追加する．

図\ref{fig:loop}(a)にCNN冒頭部分の実際のCコード，(b)に4列構成（左から第
3,2,1,0列とする）を用いた論理的多重ループ制御方法を示す．(a)の第2行から第5行
が各々第1，0，2，3列に写像されている（第4行のinit0?col:colはinit0による入力
切替え指示でありC言語としてはnop）．第0列に写像されたloop0がCGRAのバースト演
算に関与する最内ループカウンタであり，レジスタに初期値M-2が設定された後は毎
サイクル自己ループによりデクリメントされ，結果が非0の場合は第1列に写像された
loop1（初期値GRP）のデクリメント値を0に維持する．loop0が0に到達すると，第0列
の左側ソースに再度M-2を設定，第1列の右側ソースを-1に切り替えてloop1をデクリ
メント，第2列の左側ソースに再度-4を設定，第3列の右側ソースをM*4に切り替えて
rowを加算し，loop1のイタレーションが1つ進む．loop0とloop1が共に0になるとバー
スト演算が停止する．(c)は列方向マルチスレッディング適用後の提案手法であり，
以上の制御を単一unitが行う．(b)が1サイクルに4演算を並列実行するのに対し，(c)
は4倍周波数の4サイクルにより第0-3列の演算を順に実行する．パイプライン演算器
初段の自己ループにより列間データ依存に対応している．多重ループ一括実行機構に
は，同一LMMに対するread-modify-write機能も含まれる．MMおよびCNN最終段のLMMに
保存される最内ループの実行結果を一旦DDRに追い出すことなくread-modify-write機
能により累算することにより，多重ループ一括実行の効果を引き上げる．

\section{ローカルメモリ(LMM)の様々な使用方法}

\begin{figure}[htbp]
\center
\includegraphics[angle=0,origin=b,width=0.85\textwidth]{lmmmode.eps}
\caption{\label{fig:lmmmode}ローカルメモリ(LMM)の様々な使用方法}
\end{figure}

図\ref{fig:lmmmode}に，ローカルメモリ(LMM)の様々な使用方法を示す．

\begin{description} \parskip 0pc \itemsep 0pc
\item{(a)} 物理的なローカルメモリ空間を論理４列に対応して４分割したり，２分
割して論理２列で共有したり，分割せず全体で共有することができる．分割方法は混
在も可能．なお，同一LMMのプリフェッチ/ポストドレインや，ダブルバッファリング
機能を使用する場合は，最大８分割となる．

\item{(b)} 入力データをDDRからLMMに読み込んだ後，LMMから演算器入力に供給する．
ロード命令を配置する場合の，LMMの一般的な使用方法．なお，隣接LMMへのプリフェッ
チと，後述の命令写像位置シフトを組み合わせると，DDRからLMMへの転送時間を演算
時間に隠蔽できる．

\item{(c)} LMMから演算器入力に供給しながら，次回のIMAX起動に必要な入力データ
をDDRから同一LMMの別領域に読み込む，同一LMMに対して，演算用の読み出しとプリ
フェッチ用の書き込みを同時に行う使用方法．2つの空間をLMMに同居させるのでDMA
長は，LMM空間分割後のさらに半分になる．

\item{(d)} 演算結果をLMMに書き込んだ後，出力データをLMMからDDRへ書き戻す．ス
トア命令を配置する場合の，LMMの一般的な使用方法．なお，隣接LMMからのポストド
レインと，後述の命令写像位置シフトを組み合わせると，LMMからDDRへの転送時間を
演算時間に隠蔽できる．

\item{(e)} 演算結果をLMMに書き込みながら，前回起動時の出力データを同一LMMの
別領域からDDRに書き戻す．同一LMMに対して，演算結果の書き込みとポストドレイン
用の読み出しを同時に行う使用方法．2つの空間をLMMに同居させるのでDMA長は，LMM
空間分割後のさらに半分になる．

\item{(f)} DMA長に0を指定することにより，LMMとDDRの間のデータ転送を抑止でき
る．その上で，一連の演算結果をLMMに書き込みながら，前回の演算結果を同一LMMか
ら読み出して次の演算に使用できる．すなわち，LMMをダブルバッファとして使用す
ることができる．FFTやマージソートのような多段処理のパイプライン実行に利用できる．
\end{description}

\clearpage

\section{リング構造のUNIT間接続と命令写像位置シフト機能}

\begin{figure}[htbp]
\center
\epsfile{file=EMAX6RING.eps,width=0.90\textwidth}
\caption{\label{ring}24行構成の概要}
\end{figure}

課題（４）に対応するために，命令写像位置シフト機能を導入した．図\ref{ring}は，
DDRを含む24行構成の概要である．EMAX5では，各列を担当するfsmが，二分木により
接続された全てのLMMと主記憶の間のバースト転送を行うのに対し，IMAXでは，パイ
プラインメモリバスによりLMMの参照を行う．二分木のための配線を削減でき，また，
対象LMMの切替えが不要であるため，DMA転送とPIO転送を混在させて切れ目なく実行
できる．EMAX5ではDMAのみを使用していたconf，lmmi，regvの初期化に，IMAXでは
PIOを使用でき，部分的なレジスタ更新を高速化できる．DMA転送の際のUNIT間遅延は
2サイクル，演算実行時のUNIT間遅延は8サイクルである．なお，メモリバスを1列24
行構成から複数列構成に組み替えることにより，メモリバス遅延時間を短縮できる．

命令写像位置シフト機能は，リング構造を利用し，LMMの内容は移動せず，命令写像
を回転移動させる．従来コンパイラが，バースト演算起動前にホストが常時命令シフ
ト指示コードを生成していた点を見直し，バースト演算起動前に，現在のLMMのアド
レス範囲と次のバースト演算に必要なアドレス範囲を比較し，同一の場合に命令シフ
ト指示を発行しないよう改良した機能である．

\section{メモリとしてのカスケード接続機能}

\begin{figure}[htbp]
\center
\includegraphics[angle=0,origin=b,width=0.90\textwidth]{mchip.eps}
\caption{\label{fig:mchip}Cascaded peer-to-peer AXI bus for scalable
multichip CGRA}
\end{figure}

課題（５）および（６）に対応するためには，マルチチップ構成を考案した．マルチ
チップ構成によりコストダウンが図れるのは，単一チップの面積を小さくすることに
より歩留まりが改善されるためである．AMDのThreadripper（4チップのMCM）が，
IntelのCore-i9（シングルチップ）よりも低価格であることは記憶に新しい．

IMAXは，ARMv8をホストとし，標準AXIバス上のメモリデバイスとして任意の演算資源
をARMの主記憶空間上に写像できる．この際，１つの空間に複数のLMMを写像し，ホス
トからのPIO/DMAにより，多数のLMMに画像やパラメタを一斉に書き込むことも可能で
ある．プログラマはC言語によりアプリケーションを記述する際，各データ構造をど
のLMMに対応付けるかを指定し，データ構造間に演算を配置して任意のデータフロー
を形成できる．なお，データ供給パスは8列ｘ8行構成としてARMとの間の転送遅延を
低減している．さて，IMAXは，IoT向け高効率小型リニアアレイアクセラレータとし
て必要な機能を搭載し尽くしたと考えている．しかし，実用化に近づけるためには，
様々なIoTの要求性能に合わせて，スケーラブルに性能を向上できる基本構造を備え
ることが必須である．複数チップにまたがって多数のLMMへのブロードキャスト
PIO/DMAができ，演算結果の回収も複数チップにまたがってPIO/DMA転送できれば，行
列積，ステンシル計算，画像認識に必要な畳み込み演算等を見通しよく高速化できる．
ただし，GPUのように多数のメモリバスを用意して並列接続する方法は，多数のAXIバ
スを用意できないIoT向けアクセラレータでは論外である．そこで，DMAスレーブ型で
あることを利用し，同一AXIバスにカスケード接続して所要の性能に到達できるアー
キテクチャを考案した．同様のカスケード接続構成は，商用CAM-LSIの容量拡張に利
用されてきたものの，アクセラレータに適用された例はこれまでにない．これは，
GPUをはじめとする通常のアクセラレータは専らリッチなメモリバンド幅を前提とす
るDMAマスタとして設計されるためである．

図\ref{fig:mchip}に，2チップ構成の場合のunit間接続方法を示す．PIO/DMA機能を
有するホストおよび主記憶（DDR）と，チップ\#0およびチップ\#1が，1組のAXI4バス
によりカスケード接続されている．64個のunitを内蔵する各チップは，演算用に1列
構成のリング接続（各unit通過時間は8サイクル），DDR-LMM間転送用に8行x8列構成
のデータパス（同様に各unit通過時間は2サイクル）を備えている．演算用とDDR-LMM
間転送用を共有しないのは，演算と転送の同時動作に加え，チップ数増加時に問題と
なる64unit通過時間の大幅削減を狙うためである．AXI-WRITE-IFは，ホストからの書
き込み要求をチップ内および次チップに伝搬する．またAXI-READ-IFは，チップ内8列
および次チップからの応答を待ち合わせて結果を戻す．なお最終的なハード構成は，
DDR4-3600(64bit)，チップ間物理接続にXilinx社の28Gbps-GTYに相当する双方向差動
リンク(4本)x8組，片方向がチップ内スループット（後述のように
900MHzx256bit=230Gbps）に釣り合う想定である．

\begin{figure}[htbp]
\center
\epsfile{file=EMAX6SEQ.eps,width=0.76\textwidth}
\caption{\label{seq}3Chip構成の動作}
\end{figure}

図\ref{seq}は，3chip構成の動作である．DDRに格納されている入力画像は，DMAによ
り，シリアル接続された全てのIMAXに送信され，各IMAXではアドレス情報に基づき，
該当LMMが入力画像を自律的に取り込む．この際，同じアドレス情報をセットするこ
とにより，複数箇所のLMMが同時に取り込むことも可能である（図\ref{seq}(a)）．
特定のLMMに対するWriteも同様に，アドレス情報に基づき，該当LMMが自律的に取り
込む（図\ref{seq}(b)）．演算実行時は，各IMAXが独立に演算を行い，結果をLMMに
格納する（図\ref{seq}(c)）．演算結果である出力画像は，DMAにより，LMMからDDR
に読み出される（図\ref{seq}(d)）．

\begin{table}[htbp]
\center\small
\caption{\label{physinterface}物理メモリインタフェース}
%%\tabcolsep 0.2pc
\begin{tabular}{l|c|p{4.2in}}\hline\hline
信号線名称		& I/O	& 備考 \\\hline
rw			& I	& 0:read(LDDMQ/TRANSのポーリング含む), 1:write \\
ty			& I	& 0:reg/conf, 1:reg/breg, 2:reg/addr, 3:lddmq/tr, 4:lmm \\
			&       & read\&lddmq:LMMからの読み出し, write\&tr:TRへの書き込み \\
col[1:0]		& I	& 論理列番号 \\
sqi[15:0]		& I	& seq番号（最大64Kdwords）\\
avi			& I	& 0:a/dm/di無効, 1:有効 \\
a[31:5]			& I	& register/LMMのアドレス \\
dm[31:0]		& I	& register/LMMへの書き込みデータByte毎マスク \\
di[255:0]		& I	& register/LMMへの書き込みデータ \\
avo			& O	& 0:sqo/do無効, 1:有効 \\
sqo[15:0]		& O	& seq番号（最大64Kdwords）\\
do[255:0]		& O	& LMMからの読み出しデータ \\\hline
\end{tabular}
\end{table}

表\ref{physinterface}に示すCPU-IMAXの物理メモリインタフェースは，CPUが外部
メモリとしてIMAXを参照するのに必要な配線群から構成される．rw:1bitのR/W種別，
ty:2bitのregister/LMM選択，col:参照先論理列番号, sqi:16bitのseq番号，
avi:1bitのR/W要求，a:27bitのアドレス線（4dwordアライン），dm:4bitのdword単位
マスク, di:256bitのデータ線（Write），avo:1bitの読み出しデータ有効表示，
sqo:16bitのseq番号，do:256bitのデータ線（Read）が含まれる．

\clearpage

\section{詳細構造と動作}

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.98\textwidth]{IMAX1.eps}
\caption{\label{imax1}IMAXの基本UNIT構造}
\end{figure}

\begin{figure}[htbp]
\center
\epsfile{file=EMAX6EXE.eps,width=0.98\textwidth}
\caption{\label{exe}UNIT内演算器の構成とタイミング}
\end{figure}

\begin{figure}[htbp]
\center
\epsfile{file=EMAX6LMM.eps,width=0.98\textwidth}
\caption{\label{lmm}UNIT内ローカルメモリ（LMM）の構成とタイミング}
\end{figure}

\begin{figure}[htbp]
\center
\epsfile{file=EMAX6PMM.eps,width=0.98\textwidth}
\caption{\label{pmm}CPUからのローカルメモリ（LMM）直接参照}
\end{figure}

各UNITの演算器には，整数演算，単精度浮動小数点演算およびマルチメディア演算を
収容する．図\ref{exe}は，演算器に着目したレジスタ参照のタイミングチャートで
ある．4列分の演算機能を1列分のハードウェアを用いた多重実行により実現している．
レジスタ読み出しと演算を4サイクルに分割してパイプライン実行する．図\ref{lmm}
は，LMMに着目したレジスタ参照のタイミングチャートである．同様に4列分のLMM機
能を1列分のハードウェアを用いた多重実行により実現している．LMMを最大4分割し
て4つの参照をパイプライン実行する．LMMを分割しない場合，EA0/1の出力18bitがそ
のままLMMのアドレス指定に使用される．LMMを4分割する場合，EA0/1の出力18bitの
上位2bitが列番号に応じて00/01/10/11のいずれかに上書きされLMMのアドレス指定に
使用される．図\ref{pmm}は，CPUからLMMを直接参照する場合に使用するDMA/PIOのデー
タパス構成である．前行から毎サイクル，アドレス，R/W種別，書き込みの場合書き
込みデータを供給し，複数行から構成されるメモリ空間をパイプライン的に参照し，
最終行から読み出しデータを取り出す．どのUNITのLMMが目的アドレスを収容してい
るかは，アドレスと各UNIT内vAddr-range（top,len）の比較により判定し，自LMMが
一致している場合はR/W動作を行いアドレスおよびデータを次行に渡す．不一致の場
合もアドレスおよびデータを次行に渡す．

\clearpage

\begin{figure}[htbp]
\center
\epsfile{file=EMAX6MAP.eps,width=0.98\textwidth}
\caption{\label{emax6map}UNITの機能}
\end{figure}

図\ref{emax6map}(a)から(j)は，UNITに写像可能な機能である．(a)は，次の実行に
必要な主記憶からのデータをLMMにプリロードしつつ，ロード済データをLDRQ
（4dwords）/LDR（1dword）が次行の演算器に供給する．プリロードには，blk（0:ブ
ロッキング無し，1:16回連続参照毎に先頭ポインタ配列を進めるブロッキング，2:32
回連続参照毎に先頭ポインタ配列を進めるブロッキング，3:64回連続参照毎に先頭ポ
インタ配列を進めるブロッキング）およびlen（32bit単位のバースト長）を指定する．

(b)は，演算結果をSTRQ（4dwords）/STR（1dword）によりLMMにストアしつつ，前回
の実行結果を主記憶に転送する．STRQは論理的な複数列の演算結果をLMMに格納する
ために，1サイクル毎に1dwordを格納する．このため，同一行に複数のSTRQやプリロー
ドを写像することはできない．

(c)は，同一UNITにおいてLMMのread-modify-writeを行っている．top，blk，lenによ
り指定された範囲を予めLMMに格納する．その後，LDRQが読み出したデータが演算器
入力に戻され，結果が同一LMMに書き戻される．マルチスレッディング機能により，
このようなアキュムレートを写像してもパイプラインが止まることはない．

(d)は，LDRがLMMからロードするのに先立ち，top，blk，lenにより指定された範囲を
予めLMMに格納する．その後，LDRがランダムアドレス指定に基づきLMMを参照する．
top，blk，lenが同一のLDRは，LMMのデュアルポート機能が利用され，同一UNIT
に写像される．

(e)は，(d)と対をなし，演算結果をSTRによりLMMにストアしている．全てのストアが
完了後，LMMから主記憶にlenにより指定された量のデータがバースト転送される．

(f)は，トランザクションである．演算結果をTR（4dwords）によりLMMにストアしつ
つ，ARMにトランザクションに必要な4dwordsを供給する．TRは論理的な複数列の演算
結果をLMMに格納するために，1サイクル毎に1dwordを格納する．このため，同一行に
複数のTRやプリロードを写像することはできない．

(g)は，外部メモリからの直接ロード機能である．主記憶アドレス計算はEXに写像さ
れ，アドレスはLMMのdword0にキューイングされる．LMM書き込み用にはEA0を使用し，
ARMは当該AXRA（EA0と同じ値）を監視し，新規アドレスの登録を検知してLMMから主
記憶アドレスを取り出し，AXIを使用して主記憶を読み出し，LMWDにデータを送出す
る．UNITでは，LMWD→TR→BRを経由して4dwordを次行に送出する．

ところで，以上の基本機能のうち，(g)の写像に際しては，主記憶遅延に対する考慮
が必要である．プログラミング時に特殊な記述は必要としないものの，写像時には，
(g)と同一行に含まれるUNITでは，(h)のようにLMMを利用した遅延同期機構が活
性化される．同様に，レジスタ間伝搬が必要である場合，空きレジスタを使用して，
(i)（遅延同期無）や(j)（遅延同期有）のような機能が写像される．

\section{シミュレータ}

迅速かつ正確なプロトタイプ開発のためには，まず，Verilogに１対１に変換可能な
レベルの詳細シミュレータの開発が必須である．これは，Verilogシミュレータの動
作速度が極めて遅いために，大規模なアプリケーションの動作検証が困難であること，
また，Verilogシミュレータによるハードウェア設計検証のためには，レジスタ値の
正解値との比較を行うための正確な期待値が必須なことによる．IMAXの制御はARMv8
により行うため，ARMv8もシミュレーション可能なIMAXシミュレータを開発した（C言
語にて17K行の規模）．前述のアプリケーションプログラム，IMAXコンパイラ，およ
び，本シミュレータを用いて，プロトタイプシステムの設計開始前に，ハードウェア
デバッグに必須の期待値比較機能付きテストプログラムおよびVerilogシミュレーショ
ン用テストベンチを準備した．

\section{FPGAによるマルチチッププロトタイプ}

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.98\textwidth]{IMAX4.eps}
\caption{\label{imax4}4チップ構成のIMAX}
\end{figure}

8チップ構成を実機評価するためには，XILINX社製VU440を8基搭載する試験環境が必
要である．前述のシミュレータを元にVerilog記述を完成させ，シミュレータにより
生成した各レジスタの正解値とVerilogシミュレーション結果を比較してデバッグを
進めた．ハードウェア記述量は，Verilogにて11K行であった．ホストには，エッジデ
バイス向けCPUの業界標準であるARMv8を搭載したXilinx社製Zynq UltraScale+
（ZCU102），IMAXには，S2C社製Virtex Ultra Scale（S2C Single VU440 Prodigy
Logic Module）を使用した（図\ref{imax4}）．現在でも，64unit構成のIMAXを実装
可能なFPGAはVU440のみであり，ARMv8とVU440を高速シリアルリンクにより接続可能
なシステムとして本組み合わせは最適である．ただし，カタログスペック上は，
10Gbpsの高速シリアルリンクを3組束ねて30Gbpsのスループットが出せるはずなのに
対し，実際には，5Gbps x 3レーンの合計15Gbpsのスループットでしかリンクを確立
できなかった（その後8レーンに増速）．また，8チップを接続すると，LMMからの読
み出しに使用するAXI-READ動作が極めて低速であることが判明した．これは，ホスト
のDMA機能に対して十分な長さのバースト長を指定しても，ホストに内蔵されている
AXIインタフェースがAXI3互換であり，256bit幅転送の場合，8beatごとに転送が分断
され，全チップからの応答が完了するまで次のAXI-READが待たされることが原因であっ
た．そこで，変更できないホストのAXIインタフェースはAXI3のまま使用しつつ，
IMAX間では分断せずに元のバースト長を維持する機構を考案し，AXI-READの大幅な高
速化に成功した．

\section{総合評価}

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.98\textwidth]{result1.eps}
\caption{\label{result1}総合評価1 (15Gbps interface)}
\end{figure}

\begin{figure}[htbp]
\center
\includegraphics[angle=270,origin=b,width=0.98\textwidth]{result2.eps}
\caption{\label{result2}総合評価2 (40Gbps interface)}
\end{figure}

以上の経緯を経て開発が完了したIMAXの8チップ構成モデルを用いて，各種プログラ
ムを走行させた．図\ref{result1}は，5GbpsのAuroraインタフェースを3レーン使用
し(計15Gbps)，IMAXのチップ数を1から8まで変化させて測定したアプリケーションの
実行時間である．プログラム毎に，1チップ構成（動作周波数150MHz，演算器数64，
面積は28nmにて推定8.4mm2）の実行時間を基準に正規化してあり，比較のために，組
み込みGPGPU(Jetson TX2：動作周波数1.3GHz，演算器数256，演算コア部分の面積は
16nmにて推定43.6mm2)の実行時間も掲載している．行列積(mm)は7chip連結，畳み込
み(cnn)は5chip連結，距離画像(depth)は2chip連結，科学技術計算(Stencil計算)で
は，大気シミュレーション(grapes)は5chip連結，その他ステンシルは2chip連結が最
速であった．全体の傾向として，mmやcnnのようにchip間共通データが多いとマルチ
チップ化の効果が大きく，ステンシルのような単純な空間分割では効果が小さい（メ
モリバンド幅が狭いままでのマルチチップ化は必要ない）ことがわかる．総じて，エッ
ジデバイス前提の限られたメモリバンド幅(Jetson TX2の1/32)では納得できる結果が
得られた．図\ref{result2}は，5GbpsのAuroraインタフェースを8レーン使用
(40Gbps)した評価である．1.3GHz動作のJetson TX2を140MHz動作のIMAXが性能面で凌
駕していることは，CGRAのポテンシャルを十分に示している．また，IMAXが仮に28nm
のASIC化により900MHz動作した場合，Jetson TX2の1/17の面積で最大15倍(1チップ構
成のcnnの場合)の性能となり，面積あたり性能では250倍となる．消費電力が面積に
比例すると考えると，同一性能あたりの消費電力も同じ比率であると考えられる．
